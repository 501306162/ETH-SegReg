#include "forest.h"
#include <string.h>
Forest::Forest(const HyperParameters hp)
{
    m_hp = hp;
#ifdef USE_CUDA
    m_forest_d = NULL;
#endif

}

void Forest::initialize(const int numSamples)
{
    m_confidences.resize(numSamples, m_hp.numClasses);
    m_predictions.resize(numSamples);

    for (int n = 0; n < numSamples; n++) {
        for (int m = 0; m < m_hp.numClasses; m++) {
            m_confidences(n, m) = 0;
        }
    }
}

xmlNodePtr Forest::save() const
{
  xmlNodePtr node = xmlNewNode( NULL, reinterpret_cast<const xmlChar*>( "randomforest" ) );
  addIntProp( node, "numTrees", m_trees.size() );

  //xmlAddChild( node, Configurator::conf()->saveConstants() );

  std::vector<Tree>::const_iterator it( m_trees.begin() );
  std::vector<Tree>::const_iterator end( m_trees.end() );
  for ( int i = 0; it != end; ++it, ++i ) {
    xmlNodePtr stageNode = it->save();
    addIntProp( stageNode, "num", i );
    xmlAddChild( node, stageNode );
  }
  return node;
}

void Forest::train(const matrix<float>& data, const std::vector<int>& labels, bool use_gpu)
{
    // Initialize
    initialize(data.size1());

    if (m_hp.useGPU || use_gpu)
        trainByGPU(data,labels);
    else
        trainByCPU(data,labels);
}

Forest::~Forest()
{
    m_trees.clear();
#ifdef USE_CUDA
    if (m_forest_d != NULL)
        delete m_forest_d;
#endif
}

void Forest::eval(const matrix<float>& data, const std::vector<int>& labels, bool use_gpu)
{
    if (m_hp.useGPU || use_gpu)
        evalByGPU(data, labels);
    else
        evalByCPU(data, labels);
}

void Forest::save(const std::string &name)
{
    std::string saveName;
    saveName = (name == "default") ? m_hp.saveName : name;

    // now save that stuff

}

void Forest::load(const std::string &name)
{
    std::string loadName;
    loadName = (name == "default") ? m_hp.loadName : name;

    // now load that stuff

}

#ifdef USE_CUDA
void Forest::createForestMatrix()
{
  // create a forest matrix for evaluation on the gpu
  int num_cols = 0;
  if (m_hp.useRandProj)
    num_cols = 3 + 2 * m_hp.numProjFeatures + 1 + m_hp.numClasses + 1;
  else
    num_cols = 3 + 2 + 1 + m_hp.numClasses + 1;

  int num_nodes_tree = (int)pow((float)2,m_hp.maxTreeDepth + 1) - 1;
  int num_rows = m_hp.numTrees * num_nodes_tree;

  float *forest_host = new float[num_cols * num_rows];

  if (m_hp.numTrees != m_trees.size())
    throw("The number of trees trained is different from the number configured");

  std::vector<Tree>::iterator tree_it = m_trees.begin();

  for (int tree_index = 0; tree_index < m_hp.numTrees; tree_index++) {

    matrix<float> tree_matrix(num_nodes_tree, num_cols, -1.0f);
    tree_it->getTreeAsMatrix(&tree_matrix, tree_index);

    // copy data
    for (int row = 0; row < num_nodes_tree; row++) {
      forest_host[num_nodes_tree * tree_index * num_cols + row * num_cols] = (float) tree_index;
      for (int col = 1; col < num_cols; col++) {
        forest_host[num_nodes_tree * tree_index * num_cols + row * num_cols + col] = tree_matrix(row, col);
      }
    }
    tree_it++;
  }
  Cuda::HostMemoryReference<float, 2> forest_hmr(Cuda::Size<2>(num_cols, num_rows), forest_host);
  // DEBUG
  for (unsigned int row = 0; row < num_rows; row++) {
      for (unsigned int col = 0; col < num_cols; col++) {
          printf("%1.1f ", forest_host[row * num_cols + col]);
      }
      printf("\n");
  }
  m_forest_d = new Cuda::DeviceMemoryPitched<float,2>(Cuda::Size<2>(num_cols, num_rows));
  Cuda::copy(*m_forest_d, forest_hmr);

  delete[] forest_host;
}
#endif

// CPU penne code only below this line
void Forest::trainByCPU(const matrix<float>& data, const std::vector<int>& labels)
{
    std::vector<int> outOfBagVoteCount(data.size1(), 0);
    matrix<float> outOfBagConfidences(data.size1(),m_hp.numClasses);
    for( unsigned int i = 0; i < data.size1(); i++){
        for( int j = 0; j < m_hp.numClasses; j++){
            outOfBagConfidences(i,j) = 0.0;
        }
    }

    std::vector<int>::iterator preItr, preEnd;
    HyperParameters tmpHP = m_hp;
    tmpHP.verbose = false;

    if (m_hp.verbose) {
        cout << "Training a random forest with " << m_hp.numTrees << " , grab a coffee ... " << endl;
        cout << "\tTree #: ";
    }

    m_trees.clear();
    for (int i = 0; i < m_hp.numTrees; i++)
    {
        if (m_hp.verbose && !(10*i%m_hp.numTrees)) {
            cout << 100*i/m_hp.numTrees << "% " << flush;
        }

        Tree t(tmpHP);
        t.train(data,labels, m_confidences, outOfBagConfidences, outOfBagVoteCount);
        m_trees.push_back(t);
    }
    if (m_hp.verbose) {
        cout << " Done!" << endl;
    }


    double error = computeError(labels);
    double outOfBagError = computeError(labels, outOfBagConfidences, outOfBagVoteCount);

    if (m_hp.verbose) {
        cout << "\tTraining error = " << error << ", out of bag error = " << outOfBagError << endl;
    }
}

void Forest::evalByCPU(const matrix<float>& data, const std::vector<int>& labels)
{
    // Initialize
    initialize(data.size1());

    std::vector<Tree>::iterator treeItr = m_trees.begin(), treeEnd = m_trees.end();
    for (; treeItr != treeEnd; treeItr++)
        treeItr->eval(data, labels, m_confidences);

    // divide confidences by number of trees
    m_confidences *= (1.0f / m_hp.numTrees);


    double error = computeError(labels);

    if (m_hp.verbose) {
        cout << "\tTest error = " << error << endl;
    }
}

double Forest::computeError(const std::vector<int>& labels)
{
    int bestClass, nSamp = 0;
    float bestConf;
    double error = 0;
    std::vector<int>::iterator m_preItr = m_predictions.begin(), m_preEnd = m_predictions.end();
    for (; m_preItr != m_preEnd; m_preItr++, nSamp++) {
        bestClass = 0;
        bestConf = 0;
        for (int nClass = 0; nClass < (int) m_hp.numClasses; nClass++) {
            if (m_confidences(nSamp, nClass) > bestConf) {
                bestClass = nClass;
                bestConf = m_confidences(nSamp, nClass);
            }
        }

        *m_preItr = bestClass;
        if (bestClass != labels[nSamp]) {
            error++;
        }
    }
    error /= (double) m_predictions.size();

    return error;
}

double Forest::computeError(const std::vector<int>& labels, const matrix<float>& confidences,
                            const std::vector<int>& voteNum)
{
    double error = 0, bestConf;
    int sampleNum = 0, bestClass;
    for (int nSamp = 0; nSamp < (int) confidences.size1(); nSamp++) {
        bestClass = 0;
        bestConf = 0;
        if (voteNum[nSamp]) {
            sampleNum++;
            for (int nClass = 0; nClass < (int) m_hp.numClasses; nClass++) {
                if (confidences(nSamp, nClass) > bestConf) {
                    bestClass = nClass;
                    bestConf = confidences(nSamp, nClass);
                }
            }

            if (bestClass != labels[nSamp]) {
                error++;
            }
        }
    }
    error /= (double) sampleNum;

    return error;
}


// GPU spaghetti code only below this line
void Forest::trainByGPU(const matrix<float>& data, const std::vector<int>& labels)
{
    #ifdef USE_CUDA
    #endif
}

void Forest::evalByGPU(const matrix<float>& data, const std::vector<int>& labels)
{
#ifdef USE_CUDA
    // initialize
    initialize(data.size1());

    // copy input data to device
    size_t cols = data.size2();
    size_t rows = data.size1();
    Cuda::HostMemoryReference<float, 2> data_hmr(Cuda::Size<2>(cols, rows), (float*)&data.data()[0]);
    Cuda::DeviceMemoryPitched<float, 2> data_dmp(Cuda::Size<2>(cols, rows));
    Cuda::copy(data_dmp, data_hmr);

    if (m_hp.useSoftVoting) {
        // create output structure
        size_t num_samples = rows;
        Cuda::DeviceMemoryPitched<float,2> output_dmp(Cuda::Size<2>(num_samples, m_hp.numClasses));

        // evaluate tree
        if (m_hp.useRandProj) {
            iEvaluateForestDistribution(*m_forest_d, data_dmp, &output_dmp,
                m_hp.numClasses, m_hp.numTrees, m_hp.maxTreeDepth, m_hp.numProjFeatures);
        }
        else {
            iEvaluateForestDistribution(*m_forest_d, data_dmp, &output_dmp,
                m_hp.numClasses, m_hp.numTrees, m_hp.maxTreeDepth, 1);
        }

        // copy output to host memory
        Cuda::HostMemoryHeap<float,2> output_hmh(output_dmp);
        float* output_hmhp = output_hmh.getBuffer();

        // assign confidences and predictions
        for (unsigned int nSamp = 0; nSamp < num_samples; nSamp++) {
            float max_conf = 0;
            int max_class = 0;
            for (int nClass = 0; nClass < m_hp.numClasses; nClass++) {
                float conf = output_hmhp[nClass * num_samples + nSamp];
                m_confidences(nSamp,nClass) = conf;
                if (conf > max_conf) {
                    max_conf = conf;
                    max_class = nClass;
                }
            }
            m_predictions[nSamp] = max_class;
        }
    }
    else { // hard voting
        
        // create output structure
        size_t num_samples = rows;
        Cuda::DeviceMemoryLinear1D<float> output_dmp(num_samples);

        // evaluate tree
        if (m_hp.useRandProj) {
            iEvaluateForestArgMax(*m_forest_d, data_dmp, &output_dmp,
                m_hp.numClasses, m_hp.numTrees, m_hp.maxTreeDepth, m_hp.numProjFeatures);
        }
        else {
            iEvaluateForestArgMax(*m_forest_d, data_dmp, &output_dmp,
                m_hp.numClasses, m_hp.numTrees, m_hp.maxTreeDepth, 1);
        }

        // copy output to host memory
        Cuda::HostMemoryHeap<float,1> output_hmh(output_dmp);
        float* output_hmhp = output_hmh.getBuffer();

        // assign confidences and predictions
        for (unsigned int nSamp = 0; nSamp < num_samples; nSamp++) {            
            m_predictions[nSamp] = output_hmhp[nSamp];
        }
    }
    double error = computeError(labels);

    if (m_hp.verbose) {
        cout << "\tTest error = " << error << endl;
    }
#endif
}

