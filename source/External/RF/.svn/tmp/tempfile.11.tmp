#include "nodesvm.h"


<<<<<<< .mine
NodeSVM::NodeSVM(HyperParameters hp, int depth) : Node(hp, depth), m_bestFeature( -1 ),
        m_bestThreshold( 0.0 )
{
=======
NodeSVM::NodeSVM(HyperParameters hp, int depth) : Node(hp, depth) {
>>>>>>> .r62

}

<<<<<<< .mine
NodeSVM::NodeSVM(HyperParameters hp, int depth, int reset) : Node(hp, depth, reset), m_bestFeature( -1 ),
        m_bestThreshold( 0.0 )
{
=======
NodeSVM::NodeSVM(HyperParameters hp, int depth, int reset) : Node(hp, depth, reset) {
>>>>>>> .r62

}

<<<<<<< .mine

xmlNodePtr NodeSVM::saveFeature() const
{
    xmlNodePtr node = xmlNewNode( NULL, reinterpret_cast<const xmlChar*>( "feature" ) );
    addIntProp(node, "feat", m_bestFeature);
    addDoubleProp(node, "threshold", m_bestThreshold);

    return node;
}

xmlNodePtr NodeSVM::save() const
{
    xmlNodePtr node = xmlNewNode( NULL, reinterpret_cast<const xmlChar*>( "node" ) );
    xmlNewProp( node, reinterpret_cast<const xmlChar*>( "type" ),
                reinterpret_cast<const xmlChar*>( NODE_SVM ) );
    xmlAddChild(node, saveFeature());
    if (!m_isLeaf)
    {
        xmlNodePtr leftChildNode = m_leftChildNode->save();
        xmlNewProp( leftChildNode, reinterpret_cast<const xmlChar*>( "child" ),
                    reinterpret_cast<const xmlChar*>( LEFT_CHILD_NODE ) );
        xmlAddChild( node, leftChildNode );

        xmlNodePtr rightChildNode = m_rightChildNode->save();
        xmlNewProp( rightChildNode, reinterpret_cast<const xmlChar*>( "child" ),
                    reinterpret_cast<const xmlChar*>( RIGHT_CHILD_NODE ) );
        xmlAddChild( node, rightChildNode );
    }
//	if ( m_feature )
//		xmlAddChild( node, m_feature->save() );
//	else
//		addDoubleProp( node, "interstageThresh", m_interstageThreshold );

    return node;
}

std::pair<float, float> NodeSVM::calcGiniAndThreshold(const std::vector<int>& labels,
        const std::vector<std::pair<float, int> >& responses)
{
    // Initialize the counters: left takes all at the begining
    double DGini, LGini, RGini, LTotal, RTotal, bestW0 = 0, bestDGini = 1e10;
    std::vector<double> LCount(m_hp.numClasses, 0.0), RCount(m_hp.numClasses, 0.0);
=======
void NodeSVM::evalNode(const matrix<float>& data, const std::vector<int>& inBagSamples,
                       std::vector<int>& leftNodeSamples, std::vector<int>& rightNodeSamples) {
>>>>>>> .r62

}

<<<<<<< .mine
void NodeSVM::findHypotheses(const matrix<float>& data, const std::vector<int>& labels,
                             const std::vector<int>& inBagSamples, const std::vector<int>& randFeatures)
{
    std::vector<double> gini(m_hp.numRandomFeatures), thresholds(m_hp.numRandomFeatures);
    std::vector<int>::const_iterator it(randFeatures.begin());
    std::vector<int>::const_iterator end(randFeatures.end());
=======
>>>>>>> .r62

<<<<<<< .mine
    std::vector<int>::const_iterator bagIt;
    std::vector<int>::const_iterator bagEnd(inBagSamples.end());

    double bestDGini = 1e10, bestThreshold = 0;
    int bestFeature = *it;
    std::pair<float,float> curGiniThresh;
    std::vector<std::pair<float, int> > responses;

    while ( it != end )
    {
        responses.clear();
        responses.reserve(inBagSamples.size());
        bagIt = inBagSamples.begin();
        while ( bagIt != bagEnd )
        {
            responses.push_back(std::pair<float, int>(data(*bagIt,*it),*bagIt));
            ++bagIt;
        }
        sort(responses.begin(), responses.end());

        curGiniThresh = calcGiniAndThreshold(labels, responses);
        if (curGiniThresh.first < bestDGini)
        {
            bestDGini = curGiniThresh.first;
            bestFeature = *it;
            bestThreshold = curGiniThresh.second;
        }

        ++it;
    }

    m_bestFeature = bestFeature;
    m_bestThreshold = bestThreshold;
}

void NodeSVM::evalNode(const matrix<float>& data, const std::vector<int>& inBagSamples,
                       std::vector<int>& leftNodeSamples, std::vector<int>& rightNodeSamples)
{
    std::vector<int>::const_iterator it(inBagSamples.begin());
    std::vector<int>::const_iterator end(inBagSamples.end());
    while ( it != end )
    {
        if (data(*it,m_bestFeature) > m_bestThreshold)
        {
            rightNodeSamples.push_back(*it);
        }
        else
        {
            leftNodeSamples.push_back(*it);
        }
        ++it;
    }
}


=======
>>>>>>> .r62
NODE_TRAIN_STATUS NodeSVM::train(const matrix<float>& data, const std::vector<int>& labels, std::vector<int>& inBagSamples,
<<<<<<< .mine
                                 matrix<float>& confidences, std::vector<int>& predictions)
{
    bool doSplit = shouldISplit(labels,inBagSamples);

=======
                                 matrix<float>& confidences, std::vector<int>& predictions) {
>>>>>>> .r62
    NODE_TRAIN_STATUS myTrainingStatus = IS_NOT_LEAF;

<<<<<<< .mine
    if ( doSplit )
    {
        m_isLeaf = false;

        //train here the node: Select random features and evaluate them
        std::vector<int> randFeatures = randPerm(data.size2(), m_hp.numRandomFeatures);

        findHypotheses(data, labels, inBagSamples, randFeatures);
        if (m_hp.verbose)
        {
            cout << "Node #: " << m_nodeIndex << " selected feature #: " << m_bestFeature;
            cout << " and the threshold is: " << m_bestThreshold << " at depth " << m_depth << endl;
        }

        // split the data
        std::vector<int> leftNodeSamples, rightNodeSamples;
        evalNode(data,inBagSamples,leftNodeSamples,rightNodeSamples);

        // pass them to the left and right child, respectively
        m_leftChildNode = Ptr(new NodeSVM(m_hp,m_depth + 1));
        m_rightChildNode = Ptr(new NodeSVM(m_hp,m_depth + 1));

        NODE_TRAIN_STATUS leftChildStatus = m_leftChildNode->train(data,labels,leftNodeSamples,confidences,predictions);
        NODE_TRAIN_STATUS rightChildStatus= m_rightChildNode->train(data,labels,rightNodeSamples,confidences,predictions);

        if (leftChildStatus == IS_LEAF && rightChildStatus == IS_LEAF)
        {
            myTrainingStatus = CHANGE_TO_SVM;
            leftNodeSamples.clear();
            rightNodeSamples.clear();



            // Create an SVM node, and train it

            // Create left and right samples

            // Pass them to the child nodes, ask them to stay leaf
        }
    }
    else
    {
        if (m_hp.verbose)
        {
            cout << "Node #: " << m_nodeIndex << " is terminal, at depth " << m_depth << endl;
        }

        // calc confidence, labels, etc
        m_isLeaf = true;
        myTrainingStatus = IS_LEAF;
        m_nodeConf.resize(m_hp.numClasses);
        std::vector<float>::const_iterator confIt(m_nodeConf.begin());
        std::vector<float>::iterator It = m_nodeConf.begin(), end = m_nodeConf.end();
        for (; It != end; It++)
        {
            *It = 0;
        }

        std::vector<int>::const_iterator sampIt(inBagSamples.begin()), sampEnd(inBagSamples.end());
        for (; sampIt != sampEnd; sampIt++)
        {
            m_nodeConf[labels[*sampIt]]++;
        }

        int bestClass = 0, tmpN = 0;
        float bestConf = 0;
        It = m_nodeConf.begin();
        for (; It != end; It++, confIt++)
        {
            *It /= inBagSamples.size();

            if (*It > bestConf)
            {
                bestConf = *It;
                bestClass = tmpN;
            }
            tmpN++;
        }

        m_nodeLabel = bestClass;

        sampIt = inBagSamples.begin();
        for (; sampIt != sampEnd; sampIt++)
        {
            predictions[*sampIt] = m_nodeLabel;
            It = m_nodeConf.begin();
            for (int n = 0; It != end; It++, n++)
            {
                confidences(*sampIt, n) = *It;
            }
        }
    }

=======
>>>>>>> .r62
    return myTrainingStatus;
}

void NodeSVM::eval(const matrix<float>& data, const std::vector<int>& labels, const std::vector<int>& sampleIndeces,
<<<<<<< .mine
                   matrix<float>& confidences, std::vector<int>& predictions)
{
    if (m_isLeaf)
    {
        // Make predictions and confidences
        std::vector<int>::const_iterator sampIt(sampleIndeces.begin()), sampEnd(sampleIndeces.end());
        std::vector<float>::const_iterator It, end(m_nodeConf.end());
=======
                   matrix<float>& confidences, std::vector<int>& predictions) {
>>>>>>> .r62

<<<<<<< .mine
        for (; sampIt != sampEnd; sampIt++)
        {
            predictions[*sampIt] = m_nodeLabel;
            It = m_nodeConf.begin();
            for (int n = 0; It != end; It++, n++)
            {
                confidences(*sampIt, n) = *It;
            }
        }
    }
    else
    {
        // split the data

        std::vector<int> leftNodeSamples, rightNodeSamples;
        evalNode(data,sampleIndeces,leftNodeSamples,rightNodeSamples);

        m_leftChildNode->eval(data,labels,leftNodeSamples,confidences,predictions);
        m_rightChildNode->eval(data,labels,rightNodeSamples,confidences,predictions);
    }
=======
>>>>>>> .r62
}
